{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(x):\n",
    "    labels = np.unique(x)\n",
    "    result = np.zeros(shape=(x.shape[0], labels.shape[0]))\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        result[i][x[i]] = 1.0\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = [] # Features per class\n",
    "y = [] # Labels\n",
    "N = 5250 # Amount of data we want to use max: 5250\n",
    "\n",
    "# Import the features\n",
    "with open(\"traindata.txt\", \"r\") as file:\n",
    "    for line in file.readlines()[:N]:\n",
    "        features = [float(i) for i in line.split(\",\")]\n",
    "        X.append(features)\n",
    "\n",
    "# Import the labels\n",
    "with open(\"trainlabels.txt\", \"r\") as file:\n",
    "    for line in file.readlines()[:N]:\n",
    "        label = float(line.rstrip())\n",
    "        y.append(label)\n",
    "    \n",
    "# Convert data to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y, dtype=np.int32)\n",
    "y_onehot = to_onehot(y)\n",
    "\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2, random_state=42):\n",
    "   \n",
    "    # Splitting the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Further splitting the training data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size / (1 - test_size),\n",
    "                                                      random_state=random_state)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the number of components to consider when performing pca\n",
    "def num_components(X, variance_tol = 0.8):\n",
    "    # Standardize each feature of the matrix\n",
    "    x_mean = np.mean(X, axis = 0)\n",
    "    x_std = np.std(X, axis = 0)\n",
    "    Z = (X - x_mean) / x_std\n",
    "\n",
    "    # Calculate covariance matrix\n",
    "    C = np.cov(Z, rowvar=False)\n",
    "    # Calculate eigenvalues and eigenvectors and sort by size\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "    index = eigenvalues.argsort()[:: -1]\n",
    "    eigenvalues = eigenvalues[index]\n",
    "    eigenvectors = eigenvectors[:, index]\n",
    "\n",
    "    # Calculate explained variance matrix \n",
    "    explained_var = np.cumsum(eigenvalues) / np.sum(eigenvalues)\n",
    "\n",
    "    # Select number of components responsible for variance_tol% of variance\n",
    "    n_components = np.argmax(explained_var >= variance_tol) + 1\n",
    "    return Z, x_mean, x_std, n_components\n",
    "\n",
    "# Parameters are trained components, trained mean, trained standard deviation and the new inputs X\n",
    "# Changes to the PCA basis\n",
    "def convert_to_pca(components, mean, std, X):\n",
    "    Z = (X - mean)/std\n",
    "    return np.matmul(components, Z)\n",
    "\n",
    "Z, mean, std, n_components = num_components(X_train)\n",
    "# Initialize prinicipal component analysis\n",
    "pca = PCA(n_components)\n",
    "pca.fit(Z)\n",
    "components = pca.components_\n",
    "x_pca = pca.transform(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Build Model\n",
    "def build_model(n_inputs, n_outputs, h1_dims=512, h2_dims=256):\n",
    "    model =  nn.Sequential(\n",
    "        nn.Linear(n_inputs, h1_dims),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(h1_dims, h2_dims),\n",
    "        nn.ReLU(),                              \n",
    "        nn.Linear(h2_dims, n_outputs),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, h1_dims=512, h2_dims=256, name=\"NeuralNetwork\", save_dir=\"/trained_models\"):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.name = name\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.model = build_model(n_inputs, n_outputs, h1_dims, h2_dims)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.model(X)\n",
    "        return X\n",
    "    \n",
    "    def save_model(self):\n",
    "        T.save(self.state_dict, f\"{self.save_dir}/{self.name}.pth\")\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.load_state_dict(T.load(f\"{self.save_dir}/{name}.pth\"))\n",
    "\n",
    "\n",
    "\n",
    "labels = np.unique(y) # Labels of our data (0 - 20)\n",
    "\n",
    "n_inputs = X_train.shape[1]\n",
    "h1_dims = 2 * n_inputs # Number of nodes for the 1st hidden layer\n",
    "h2_dims = n_inputs # Number of nodes for the 2nd hidden layer\n",
    "n_outputs = labels.shape[0] # 21 labels\n",
    "\n",
    "# Initialize the model\n",
    "net = NeuralNetwork(n_inputs=n_inputs, n_outputs=n_outputs, h1_dims=h1_dims, h2_dims=h2_dims, name=\"NN-v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.085059  [   50/ 3150]\n",
      "loss: 0.091429  [  300/ 3150]\n",
      "loss: 0.091429  [  550/ 3150]\n",
      "loss: 0.080000  [  800/ 3150]\n",
      "loss: 0.091429  [ 1050/ 3150]\n",
      "loss: 0.093333  [ 1300/ 3150]\n",
      "loss: 0.087619  [ 1550/ 3150]\n",
      "loss: 0.095238  [ 1800/ 3150]\n",
      "loss: 0.093333  [ 2050/ 3150]\n",
      "loss: 0.089524  [ 2300/ 3150]\n",
      "loss: 0.091429  [ 2550/ 3150]\n",
      "loss: 0.093333  [ 2800/ 3150]\n",
      "loss: 0.091429  [ 3050/ 3150]\n",
      "Finished training\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 0.000088 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.089524  [   50/ 3150]\n",
      "loss: 0.091429  [  300/ 3150]\n",
      "loss: 0.091429  [  550/ 3150]\n",
      "loss: 0.080000  [  800/ 3150]\n",
      "loss: 0.091429  [ 1050/ 3150]\n",
      "loss: 0.093333  [ 1300/ 3150]\n",
      "loss: 0.087619  [ 1550/ 3150]\n",
      "loss: 0.095238  [ 1800/ 3150]\n",
      "loss: 0.093333  [ 2050/ 3150]\n",
      "loss: 0.089524  [ 2300/ 3150]\n",
      "loss: 0.091429  [ 2550/ 3150]\n",
      "loss: 0.093333  [ 2800/ 3150]\n",
      "loss: 0.091429  [ 3050/ 3150]\n",
      "Finished training\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 0.000088 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.089524  [   50/ 3150]\n",
      "loss: 0.091429  [  300/ 3150]\n",
      "loss: 0.091429  [  550/ 3150]\n",
      "loss: 0.080000  [  800/ 3150]\n",
      "loss: 0.091429  [ 1050/ 3150]\n",
      "loss: 0.093333  [ 1300/ 3150]\n",
      "loss: 0.087619  [ 1550/ 3150]\n",
      "loss: 0.095238  [ 1800/ 3150]\n",
      "loss: 0.093333  [ 2050/ 3150]\n",
      "loss: 0.089524  [ 2300/ 3150]\n",
      "loss: 0.091429  [ 2550/ 3150]\n",
      "loss: 0.093333  [ 2800/ 3150]\n",
      "loss: 0.091429  [ 3050/ 3150]\n",
      "Finished training\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 0.000088 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.089524  [   50/ 3150]\n",
      "loss: 0.091429  [  300/ 3150]\n",
      "loss: 0.091429  [  550/ 3150]\n",
      "loss: 0.080000  [  800/ 3150]\n",
      "loss: 0.091429  [ 1050/ 3150]\n",
      "loss: 0.093333  [ 1300/ 3150]\n",
      "loss: 0.087619  [ 1550/ 3150]\n",
      "loss: 0.095238  [ 1800/ 3150]\n",
      "loss: 0.093333  [ 2050/ 3150]\n",
      "loss: 0.089524  [ 2300/ 3150]\n",
      "loss: 0.091429  [ 2550/ 3150]\n",
      "loss: 0.093333  [ 2800/ 3150]\n",
      "loss: 0.091429  [ 3050/ 3150]\n",
      "Finished training\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 0.000088 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.089524  [   50/ 3150]\n",
      "loss: 0.091429  [  300/ 3150]\n",
      "loss: 0.091429  [  550/ 3150]\n",
      "loss: 0.080000  [  800/ 3150]\n",
      "loss: 0.091429  [ 1050/ 3150]\n",
      "loss: 0.093333  [ 1300/ 3150]\n",
      "loss: 0.087619  [ 1550/ 3150]\n",
      "loss: 0.095238  [ 1800/ 3150]\n",
      "loss: 0.093333  [ 2050/ 3150]\n",
      "loss: 0.089524  [ 2300/ 3150]\n",
      "loss: 0.091429  [ 2550/ 3150]\n",
      "loss: 0.093333  [ 2800/ 3150]\n",
      "loss: 0.091429  [ 3050/ 3150]\n",
      "Finished training\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 0.000088 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.089524  [   50/ 3150]\n",
      "loss: 0.091429  [  300/ 3150]\n",
      "loss: 0.091429  [  550/ 3150]\n",
      "loss: 0.080000  [  800/ 3150]\n",
      "loss: 0.091429  [ 1050/ 3150]\n",
      "loss: 0.093333  [ 1300/ 3150]\n",
      "loss: 0.087619  [ 1550/ 3150]\n",
      "loss: 0.095238  [ 1800/ 3150]\n",
      "loss: 0.093333  [ 2050/ 3150]\n",
      "loss: 0.089524  [ 2300/ 3150]\n",
      "loss: 0.091429  [ 2550/ 3150]\n",
      "loss: 0.093333  [ 2800/ 3150]\n",
      "loss: 0.091429  [ 3050/ 3150]\n",
      "Finished training\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 0.000088 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.089524  [   50/ 3150]\n",
      "loss: 0.091429  [  300/ 3150]\n",
      "loss: 0.091429  [  550/ 3150]\n",
      "loss: 0.080000  [  800/ 3150]\n",
      "loss: 0.091429  [ 1050/ 3150]\n",
      "loss: 0.093333  [ 1300/ 3150]\n",
      "loss: 0.087619  [ 1550/ 3150]\n",
      "loss: 0.095238  [ 1800/ 3150]\n",
      "loss: 0.093333  [ 2050/ 3150]\n",
      "loss: 0.089524  [ 2300/ 3150]\n",
      "loss: 0.091429  [ 2550/ 3150]\n",
      "loss: 0.093333  [ 2800/ 3150]\n",
      "loss: 0.091429  [ 3050/ 3150]\n",
      "Finished training\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 0.000088 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.089524  [   50/ 3150]\n",
      "loss: 0.091429  [  300/ 3150]\n",
      "loss: 0.091429  [  550/ 3150]\n",
      "loss: 0.080000  [  800/ 3150]\n",
      "loss: 0.091429  [ 1050/ 3150]\n",
      "loss: 0.093333  [ 1300/ 3150]\n",
      "loss: 0.087619  [ 1550/ 3150]\n",
      "loss: 0.095238  [ 1800/ 3150]\n",
      "loss: 0.093333  [ 2050/ 3150]\n",
      "loss: 0.089524  [ 2300/ 3150]\n",
      "loss: 0.091429  [ 2550/ 3150]\n",
      "loss: 0.093333  [ 2800/ 3150]\n",
      "loss: 0.091429  [ 3050/ 3150]\n",
      "Finished training\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 0.000088 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.089524  [   50/ 3150]\n",
      "loss: 0.091429  [  300/ 3150]\n",
      "loss: 0.091429  [  550/ 3150]\n",
      "loss: 0.080000  [  800/ 3150]\n",
      "loss: 0.091429  [ 1050/ 3150]\n",
      "loss: 0.093333  [ 1300/ 3150]\n",
      "loss: 0.087619  [ 1550/ 3150]\n",
      "loss: 0.095238  [ 1800/ 3150]\n",
      "loss: 0.093333  [ 2050/ 3150]\n",
      "loss: 0.089524  [ 2300/ 3150]\n",
      "loss: 0.091429  [ 2550/ 3150]\n",
      "loss: 0.093333  [ 2800/ 3150]\n",
      "loss: 0.091429  [ 3050/ 3150]\n",
      "Finished training\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 0.000088 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.089524  [   50/ 3150]\n",
      "loss: 0.091429  [  300/ 3150]\n",
      "loss: 0.091429  [  550/ 3150]\n",
      "loss: 0.080000  [  800/ 3150]\n",
      "loss: 0.091429  [ 1050/ 3150]\n",
      "loss: 0.093333  [ 1300/ 3150]\n",
      "loss: 0.087619  [ 1550/ 3150]\n",
      "loss: 0.095238  [ 1800/ 3150]\n",
      "loss: 0.093333  [ 2050/ 3150]\n",
      "loss: 0.089524  [ 2300/ 3150]\n",
      "loss: 0.091429  [ 2550/ 3150]\n",
      "loss: 0.093333  [ 2800/ 3150]\n",
      "loss: 0.091429  [ 3050/ 3150]\n",
      "Finished training\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 0.000088 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Global Variables\n",
    "epochs = 10\n",
    "learning_rate = 1e-5\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def test_model(model, X_test, y_test, loss_func):\n",
    "    size = X_test.shape[1]\n",
    "\n",
    "    model.eval()\n",
    "    with T.no_grad():\n",
    "        X = T.from_numpy(X_test).to(T.float32)\n",
    "        y_tensor = T.Tensor(y_test).to(T.float)\n",
    "\n",
    "        y_pred = model.forward(X)\n",
    "\n",
    "        test_loss = loss_func(y_pred, y_tensor)\n",
    "\n",
    "        correct = (y_pred.argmax(1) == y_tensor.argmax(1)).type(T.float).sum().item()\n",
    "        \n",
    "        test_loss /= size\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, loss_func, optimizer):\n",
    "    size = len(X_train)\n",
    "    batch_size = 50\n",
    "\n",
    "    for i in range(size//batch_size):\n",
    "        start = batch_size * i\n",
    "        end = start + batch_size\n",
    "        X = T.from_numpy(X_train[start:end]).to(T.float32)\n",
    "        y_true = T.Tensor(y_train[start:end]).to(T.float)\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_func(y_pred, y_true)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i * batch_size) % 250 == 0:\n",
    "            loss, current = loss.item(), (i + 1) * batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "\n",
    "    train_model(net, X_train, y_train, loss_func, optimizer)\n",
    "    print('Finished training')\n",
    "    test_model(net, X_test, y_test, loss_func)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
