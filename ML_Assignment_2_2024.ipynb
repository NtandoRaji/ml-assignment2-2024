{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System:\n",
      "    python: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0]\n",
      "executable: /home/vmuser/anaconda3/bin/python\n",
      "   machine: Linux-5.15.0-58-generic-x86_64-with-glibc2.35\n",
      "\n",
      "Python dependencies:\n",
      "      sklearn: 1.4.2\n",
      "          pip: 22.3.1\n",
      "   setuptools: 65.6.3\n",
      "        numpy: 1.23.5\n",
      "        scipy: 1.10.0\n",
      "       Cython: None\n",
      "       pandas: 1.5.2\n",
      "   matplotlib: 3.7.0\n",
      "       joblib: 1.4.2\n",
      "threadpoolctl: 2.2.0\n",
      "\n",
      "Built with OpenMP: True\n",
      "\n",
      "threadpoolctl info:\n",
      "       filepath: /home/vmuser/anaconda3/lib/libmkl_rt.so.1\n",
      "         prefix: libmkl_rt\n",
      "       user_api: blas\n",
      "   internal_api: mkl\n",
      "        version: 2021.4-Product\n",
      "    num_threads: 4\n",
      "threading_layer: intel\n",
      "\n",
      "       filepath: /home/vmuser/anaconda3/lib/libiomp5.so\n",
      "         prefix: libiomp\n",
      "       user_api: openmp\n",
      "   internal_api: openmp\n",
      "        version: None\n",
      "    num_threads: 8\n",
      "\n",
      "       filepath: /home/vmuser/anaconda3/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n",
      "         prefix: libgomp\n",
      "       user_api: openmp\n",
      "   internal_api: openmp\n",
      "        version: None\n",
      "    num_threads: 8\n"
     ]
    }
   ],
   "source": [
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_tol = 150\n",
    "cov_tol = 0.66\n",
    "n_clusters = 21\n",
    "N = 5250 # Amount of data we want to use max: 5250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(x):\n",
    "    labels = np.unique(x)\n",
    "    result = np.zeros(shape=(x.shape[0], labels.shape[0]))\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        result[i][x[i]] = 1.0\n",
    "\n",
    "    return result\n",
    "\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2):\n",
    "   \n",
    "    # Splitting the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    # Further splitting the training data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size / (1 - test_size))\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # Features per class\n",
    "y = [] # Labels\n",
    "\n",
    "# Import the features\n",
    "with open(\"traindata.txt\", \"r\") as file:\n",
    "    for line in file.readlines()[:N]:\n",
    "        features = [float(i) for i in line.split(\",\")]\n",
    "        X.append(features)\n",
    "\n",
    "# Import the labels\n",
    "with open(\"trainlabels.txt\", \"r\") as file:\n",
    "    for line in file.readlines()[:N]:\n",
    "        label = float(line.rstrip())\n",
    "        y.append(label)\n",
    "    \n",
    "# Convert data to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y, dtype=np.int32)\n",
    "\n",
    "# Raw data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, 0.2, 0.1)\n",
    "# Onehot data\n",
    "y_train_onehot, y_test_onehot, y_val_onehot = to_onehot(y_train), to_onehot(y_test), to_onehot(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the number of components to consider when performing pca\n",
    "def num_components(X, variance_tol = 0.8):\n",
    "    # Standardize each feature of the matrix\n",
    "    x_mean = np.mean(X, axis = 0)\n",
    "    x_std = np.std(X, axis = 0)\n",
    "    Z = normalize_data(X, x_mean, x_std)\n",
    "\n",
    "    # Calculate covariance matrix\n",
    "    C = np.cov(Z, rowvar=False)\n",
    "    # Calculate eigenvalues and eigenvectors and sort by size\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "    index = eigenvalues.argsort()[:: -1]\n",
    "    eigenvalues = eigenvalues[index]\n",
    "    eigenvectors = eigenvectors[:, index]\n",
    "\n",
    "    # Calculate explained variance matrix \n",
    "    explained_var = np.cumsum(eigenvalues) / np.sum(eigenvalues)\n",
    "\n",
    "    # Select number of components responsible for variance_tol% of variance\n",
    "    n_components = np.argmax(explained_var >= variance_tol) + 1\n",
    "    return Z, x_mean, x_std, n_components\n",
    "\n",
    "def normalize_data(X, mean, std):\n",
    "    return (X - mean)/std\n",
    "\n",
    "def convert_to_pca(components, mean, std, X):\n",
    "    Z = normalize_data(X, mean, std)\n",
    "    return np.transpose(np.matmul(components, np.transpose(Z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, mean, std, n_components = num_components(X_train, cov_tol)\n",
    "# Initialize prinicipal component analysis\n",
    "pca = PCA(n_components)\n",
    "pca.fit(Z)\n",
    "components = pca.components_\n",
    "# Data in PCA form\n",
    "X_train_PCA = pca.transform(Z)\n",
    "X_test_PCA = convert_to_pca(components, mean, std, X_test)\n",
    "X_val_PCA = convert_to_pca(components, mean, std, X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3675, 47)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_PCA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"pca_utils/pca_components\", components)\n",
    "np.save(\"pca_utils/X_mean\", mean)\n",
    "np.save(\"pca_utils/X_std\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_cover(X, std_tol = 500):\n",
    "    x_mean = np.mean(X, axis = 0)\n",
    "    x_std = np.mean(X, axis = 0)\n",
    "    feature_cover = np.zeros(len(x_mean), dtype = bool)\n",
    "\n",
    "    for i in range(len(x_mean)):\n",
    "        if np.abs(x_std[i]) < std_tol:\n",
    "            feature_cover[i] = True    \n",
    "\n",
    "    return feature_cover\n",
    "\n",
    "# generate_feature_cover_correlation(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cover = generate_feature_cover(X_train, std_tol)\n",
    "# Data with feature cover\n",
    "X_train_cover, X_test_cover, X_val_cover = X_train[:, feature_cover],  X_test[:, feature_cover],  X_val[:, feature_cover]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, mean, std, n_components = num_components(X_train_cover, cov_tol)\n",
    "# Initialize prinicipal component analysis\n",
    "pca = PCA(n_components)\n",
    "pca.fit(Z)\n",
    "components = pca.components_\n",
    "\n",
    "# Data in covered PCA form\n",
    "X_train_cover_PCA = pca.transform(Z)\n",
    "X_test_cover_PCA = convert_to_pca(components, mean, std, X_test_cover)\n",
    "X_val_cover_PCA = convert_to_pca(components, mean, std, X_val_cover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot_matrix(X_train, y_train, n = 5):\n",
    "    combined_data = np.hstack((np.transpose([y_train]), X_train[:, 0: n]))\n",
    "    df = pd.DataFrame(combined_data)\n",
    "    sns.pairplot(df, hue=0)\n",
    "\n",
    "def get_correlations(X_train):\n",
    "    df = pd.DataFrame(X_train)\n",
    "    correlations = df.corr()\n",
    "    return correlations\n",
    "\n",
    "def heatmap(X_train, n = 5):\n",
    "    correlations = get_correlations(X_train)\n",
    "\n",
    "    sns.heatmap(correlations, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scatterplot matrices\n",
    "# # Visualize raw data\n",
    "# scatterplot_matrix(X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize covered data\n",
    "# scatterplot_matrix(X_train_cover, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize PCA data\n",
    "# scatterplot_matrix(X_train_PCA, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap(X_train, len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap(X_train_PCA, len(X_train_PCA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(X_train, X_test, n_clusters = 21):\n",
    "      kmeans = KMeans(n_clusters)\n",
    "      kmeans.fit(X_train)\n",
    "      centroids = kmeans.cluster_centers_\n",
    "      y_pred = kmeans.predict(X_test)\n",
    "      return y_pred, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data:\n",
      "Number of mislabeled points out of a total 1050 points : 1007\n"
     ]
    }
   ],
   "source": [
    "# Cluster raw data\n",
    "y_pred, centroids = cluster(X_train, X_test, n_clusters)\n",
    "\n",
    "print(\"Raw Data:\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covered Data:\n",
      "Number of mislabeled points out of a total 1050 points : 994\n"
     ]
    }
   ],
   "source": [
    "# Cluster covered data\n",
    "y_pred, centroids = cluster(X_train_cover, X_test_cover, n_clusters)\n",
    "\n",
    "print(\"Covered Data:\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Data:\n",
      "Number of mislabeled points out of a total 1050 points : 1017\n"
     ]
    }
   ],
   "source": [
    "# Cluster PCA data\n",
    "y_pred, centroids = cluster(X_train_PCA, X_test_PCA, n_clusters)\n",
    "\n",
    "print(\"PCA Data:\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(X_train, y_train, X_test):\n",
    "      gnb = GaussianNB()\n",
    "      y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "      return y_pred\n",
    "\n",
    "def make_confusion_matrix_standard(y_test, y_pred):\n",
    "      c_matrix = np.zeros((len(np.unique(y_test)), len(np.unique(y_test))))\n",
    "      for i in range(len(y_test)):\n",
    "            c_matrix[y_test[i], y_pred[i]] += 1\n",
    "      return c_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data:\n",
      "Number of mislabeled points out of a total 1050 points : 786\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes with raw data\n",
    "y_pred = naive_bayes(X_train, y_train, X_test)\n",
    "print(\"Raw Data:\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covered Data:\n",
      "Number of mislabeled points out of a total 1050 points : 786\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes with covered data\n",
    "y_pred = naive_bayes(X_train_cover, y_train, X_test_cover)\n",
    "print(\"Covered Data:\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Data:\n",
      "Number of mislabeled points out of a total 1050 points : 626\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes with PCA data\n",
    "y_pred_nb = naive_bayes(X_train_PCA, y_train, X_test_PCA)\n",
    "print(\"PCA Data:\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "\n",
    "      % (X_test.shape[0], (y_test != y_pred_nb).sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled test points out of a total 1050 points : 492\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 5000\n",
    "clf = RandomForestClassifier(n_estimators=n_estimators, n_jobs = -1)\n",
    "\n",
    "clf.fit(X_train_PCA, y_train)\n",
    "y_pred_tree = clf.predict(X_test_PCA)\n",
    "\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\"\n",
    "      % (X_test_PCA.shape[0], (y_test != y_pred_tree).sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"clf.pkl\", \"wb\") as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled test points out of a total 1050 points : 471\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "\n",
    "with open(\"clf.pkl\", \"rb\") as file:\n",
    "      clf2 = pickle.load(file)\n",
    "\n",
    "y_pred_tree = clf2.predict(X_test_PCA)\n",
    "\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\"\n",
    "      % (X_test_PCA.shape[0], (y_test != y_pred_tree).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch as T\n",
    "# import torch.nn as nn\n",
    "\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self, n_inputs, n_outputs, p_dropout=0.20, save_dir=\"./models\"):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "#         self.save_dir = save_dir\n",
    "\n",
    "#         activation = nn.ReLU()\n",
    "#         dropout = nn.Dropout(p=p_dropout)\n",
    "\n",
    "#         self.network = nn.Sequential(\n",
    "#             nn.Linear(in_features=n_inputs, out_features=n_inputs * 3),\n",
    "#             activation,\n",
    "#             dropout,\n",
    "#             nn.Linear(in_features=n_inputs * 3, out_features=n_inputs * 2),\n",
    "#             activation,\n",
    "#             dropout,\n",
    "#             nn.Linear(in_features=n_inputs * 2, out_features=n_inputs),\n",
    "#             activation,\n",
    "#             dropout,\n",
    "#             nn.Linear(in_features=n_inputs, out_features=n_outputs),\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         logits = self.network(X)\n",
    "#         return logits\n",
    "    \n",
    "#     def save(self, name):\n",
    "#         T.save(self.state_dict(), f\"{self.save_dir}/{name}.pth\")\n",
    "\n",
    "#     def load(self, name):\n",
    "#         self.load_state_dict(T.load(f\"{self.save_dir}/{name}.pth\"))\n",
    "\n",
    "\n",
    "# def get_n_inputs(X_train):\n",
    "#     return X_train.shape[1] \n",
    "\n",
    "# n_inputs = get_n_inputs(X_train_PCA)\n",
    "\n",
    "# n_outputs = 21 # 21 labels\n",
    "\n",
    "# # Move a tensor to the GPU\n",
    "# device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Initialize the model\n",
    "# model_1 = NeuralNetwork(n_inputs=n_inputs, n_outputs=n_outputs, p_dropout=0.2).to(device)\n",
    "# model_2 = NeuralNetwork(n_inputs=n_inputs, n_outputs=n_outputs, p_dropout=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_confusion_matrix(y_pred, y_true, labels):\n",
    "#     N = labels.shape[0]\n",
    "#     matrix = [[0] * (N + 1) for _ in range(N + 1)]\n",
    "\n",
    "#     matrix[0][0] = \" \"\n",
    "#     for i in range(1, N):\n",
    "#         matrix[i][0] = f\"{i}\"\n",
    "#         matrix[0][i] = f\"{i}\"\n",
    "\n",
    "#     for i in range(len(y_pred)):\n",
    "#         matrix[round(y_pred[i]) + 1][y_true[i] + 1] += 1\n",
    "\n",
    "#     for i in range(N):\n",
    "#         print(\" \".join(map(str, matrix[i])))\n",
    "\n",
    "#     return sum([matrix[i + 1][i + 1] for i in range(2)]) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_model(model1, model2, X_val, y_val, criterion):\n",
    "#     size = len(y_val)\n",
    "\n",
    "#     model1.eval()\n",
    "#     model2.eval()\n",
    "\n",
    "#     with T.no_grad():\n",
    "#         X = T.from_numpy(X_val).to(T.float32).to(device)\n",
    "#         y_true = T.Tensor(y_val).to(T.float).to(device)\n",
    "\n",
    "#         logits_1 = model1.forward(X)\n",
    "#         logits_2 = model2.forward(X)\n",
    "\n",
    "#         loss_1, loss_2 = criterion(logits_1, logits_2, y_true, 0.2)\n",
    "\n",
    "#         correct_1 = (logits_1.argmax(1) == y_true.argmax(1)).type(T.float).sum().item()\n",
    "#         correct_2 = (logits_2.argmax(1) == y_true.argmax(1)).type(T.float).sum().item()\n",
    "        \n",
    "#         loss_1 /= size\n",
    "#         loss_2 /= size\n",
    "#         accuracy_1 = correct_1/size\n",
    "#         accuracy_2 = correct_2/size\n",
    "#         print(f\"Validation Error (Model 1): \\n Accuracy: {(100 * (accuracy_1)):>0.1f}%, Avg loss: {loss_1:>8f}\")\n",
    "#         print(f\"Validation Error (Model 2): \\n Accuracy: {(100 * (accuracy_2)):>0.1f}%, Avg loss: {loss_2:>8f} \\n\")\n",
    "    \n",
    "#     return accuracy_1, accuracy_2, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model_1, model_2, X_train, y_train, criterion, optimizer_1, optimizer_2):\n",
    "#     size = len(X_train)\n",
    "#     batch_size = 105\n",
    "\n",
    "#     #Prevents model from memorizing the position of data\n",
    "#     indices = np.random.randint(0, size, size)\n",
    "\n",
    "#     model_1.train()\n",
    "#     model_2.train()\n",
    "\n",
    "#     for i in range(size//batch_size):\n",
    "#         start = batch_size * i\n",
    "#         end = start + batch_size\n",
    "\n",
    "#         X = T.from_numpy(X_train[indices[start:end]]).to(T.float32).to(device)\n",
    "#         y_true = T.Tensor(y_train[indices[start:end]]).to(T.float).to(device)\n",
    "\n",
    "#         logits_1 = model_1.forward(X)\n",
    "#         logits_2 = model_2.forward(X)\n",
    "\n",
    "#         loss_1, loss_2 = criterion(logits_1, logits_2, y_true, 0.2)\n",
    "\n",
    "#         # Gradiant Descent using Adam optimizer for best performance\n",
    "#         optimizer_1.zero_grad()\n",
    "#         loss_1.backward()\n",
    "#         optimizer_1.step()\n",
    "\n",
    "#         optimizer_2.zero_grad()\n",
    "#         loss_2.backward()\n",
    "#         optimizer_2.step()\n",
    "\n",
    "#         if (i * batch_size) % 420 == 0:\n",
    "#             loss_1, loss_2, current = loss_1.item(), loss_2.item(), (i + 1) * batch_size\n",
    "#             print(f\"loss1: {loss_1:>7f} loss2: {loss_2:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch as T\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Loss functions\n",
    "# def loss_coteaching(y_1, y_2, t, forget_rate):\n",
    "#     loss_1 = F.cross_entropy(y_1, t, reduce = False)\n",
    "#     ind_1_sorted = T.argsort(loss_1.data)\n",
    "#     loss_1_sorted = loss_1[ind_1_sorted]\n",
    "\n",
    "#     loss_2 = F.cross_entropy(y_2, t, reduce = False)\n",
    "#     ind_2_sorted = T.argsort(loss_2.data)\n",
    "#     loss_2_sorted = loss_2[ind_2_sorted]\n",
    "\n",
    "#     remember_rate = 1 - forget_rate\n",
    "#     num_remember = int(remember_rate * len(loss_1_sorted))\n",
    "\n",
    "\n",
    "#     ind_1_update=ind_1_sorted[:num_remember]\n",
    "#     ind_2_update=ind_2_sorted[:num_remember]\n",
    "#     # exchange\n",
    "#     loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update])\n",
    "#     loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update])\n",
    "\n",
    "#     return T.sum(loss_1_update)/num_remember, T.sum(loss_2_update)/num_remember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# # Global Variables\n",
    "# epochs = 5_000\n",
    "# learning_rate = 1e-4\n",
    "\n",
    "# # Load Pre-Trained Models\n",
    "# # model_1.load(\"NeuralNetwork-1_acc-50.29_loss-0.000003\")\n",
    "# # model_2.load(\"NeuralNetwork-2_acc-50.38_loss-0.000003\")\n",
    "\n",
    "# criterion = loss_coteaching # Co-teaching loss function\n",
    "# optimizer_1 = optim.Adam(model_1.parameters(), lr=learning_rate)\n",
    "# optimizer_2 = optim.Adam(model_2.parameters(), lr=learning_rate)\n",
    "\n",
    "# best_accuracy = 0.50\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "\n",
    "#     train_model(model_1, model_2, X_train_PCA, y_train_onehot, criterion, optimizer_1, optimizer_2)\n",
    "#     print('Finished training')\n",
    "    \n",
    "#     accuracy_1, accuracy_2, loss_1, loss_2 = validate_model(model_1, model_2, X_val_PCA, y_val_onehot, criterion)\n",
    "\n",
    "#     if max(accuracy_1, accuracy_2) > best_accuracy:\n",
    "#         print(f\"[+] Saving Model...\")\n",
    "\n",
    "#         model_1.save(f\"NeuralNetwork-1_acc-{accuracy_1 * 100:.2f}_loss-{loss_1:>8f}\")\n",
    "#         model_2.save(f\"NeuralNetwork-2_acc-{accuracy_2 * 100:.2f}_loss-{loss_2:>8f}\")\n",
    "#         best_accuracy = max(accuracy_1, accuracy_2)\n",
    "\n",
    "#         print(f\"[!] Models Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
