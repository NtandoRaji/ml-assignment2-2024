{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(x):\n",
    "    labels = np.unique(x)\n",
    "    result = np.zeros(shape=(x.shape[0], labels.shape[0]))\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        result[i][x[i]] = 1.0\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_feature_cover(X, std_tol = 500):\n",
    "    x_mean = np.mean(X, axis = 0)\n",
    "    x_std = np.mean(X, axis = 0)\n",
    "    feature_cover = np.zeros(len(x_mean), dtype = bool)\n",
    "\n",
    "    for i in range(len(x_mean)):\n",
    "        if np.abs(x_std[i]) > std_tol:\n",
    "            feature_cover[i] = True    \n",
    "\n",
    "    return feature_cover\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2, random_state=42):\n",
    "   \n",
    "    # Splitting the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Further splitting the training data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size / (1 - test_size),\n",
    "                                                      random_state=random_state)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [] # Features per class\n",
    "training_labels = [] # Labels\n",
    "testing_data = [] # Features per class\n",
    "testing_labels = [] # Labels\n",
    "N = 5250 # Amount of data we want to use max: 5250\n",
    "\n",
    "# Import the features\n",
    "with open(\"traindata.txt\", \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        features = [float(i) for i in line.split(\",\")]\n",
    "        training_data.append(features)\n",
    "\n",
    "with open(\"testdata.txt\", \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        features = [float(i) for i in line.split(\",\")]\n",
    "        testing_data.append(features)\n",
    "\n",
    "\n",
    "# Import the labels\n",
    "with open(\"trainlabels.txt\", \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        label = float(line.rstrip())\n",
    "        training_labels.append(label)\n",
    "\n",
    "with open(\"targetlabels.txt\", \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        label = float(line.rstrip())\n",
    "        testing_labels.append(label)\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "# X = np.array(training_data)\n",
    "# y = to_onehot(np.array(training_labels, dtype=np.int64))\n",
    "\n",
    "# feature_cover = generate_feature_cover(X, 1000)\n",
    "\n",
    "X_train = np.load(\"augmented_traindata.npy\")\n",
    "y_train = to_onehot(np.load(\"augmented_trainlabels.npy\"))\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(np.array(testing_data), to_onehot(np.array(testing_labels, dtype=np.int64)), test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the number of components to consider when performing pca\n",
    "def num_components(X, variance_tol = 0.8):\n",
    "    # Standardize each feature of the matrix\n",
    "    x_mean = np.mean(X, axis = 0)\n",
    "    x_std = np.std(X, axis = 0)\n",
    "    Z = (X - x_mean) / x_std\n",
    "\n",
    "    # Calculate covariance matrix\n",
    "    C = np.cov(Z, rowvar=False)\n",
    "    # Calculate eigenvalues and eigenvectors and sort by size\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "    index = eigenvalues.argsort()[:: -1]\n",
    "    eigenvalues = eigenvalues[index]\n",
    "    eigenvectors = eigenvectors[:, index]\n",
    "\n",
    "    # Calculate explained variance matrix \n",
    "    explained_var = np.cumsum(eigenvalues) / np.sum(eigenvalues)\n",
    "\n",
    "    # Select number of components responsible for variance_tol% of variance\n",
    "    n_components = np.argmax(explained_var >= variance_tol) + 1\n",
    "    return Z, x_mean, x_std, n_components\n",
    "\n",
    "# Parameters are trained components, trained mean, trained standard deviation and the new inputs X\n",
    "# Changes to the PCA basis\n",
    "def convert_to_pca(components, mean, std, X):\n",
    "    Z = (X - mean)/std\n",
    "    return Z @ components.transpose()\n",
    "\n",
    "Z, mean, std, n_components = num_components(X_train, 0.67)\n",
    "# Initialize prinicipal component analysis\n",
    "pca = PCA(n_components, random_state=453)\n",
    "pca.fit(Z)\n",
    "components = pca.components_\n",
    "X_train_PCA = pca.transform(Z)\n",
    "temp = pca.transform(X_test)\n",
    "X_test_PCA = convert_to_pca(components, mean, std, X_test)\n",
    "X_val_PCA = convert_to_pca(components, mean, std, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"pca_utils/pca_components\", components)\n",
    "np.save(\"pca_utils/X_mean\", mean)\n",
    "np.save(\"pca_utils/X_std\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5922, 49)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_PCA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, p_dropout=0.20,num_groups =32, save_dir=\"./models\"):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        activation = nn.ReLU()\n",
    "        dropout = nn.AlphaDropout(p=p_dropout)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(in_features=n_inputs, out_features=1024),\n",
    "            nn.GroupNorm(num_groups,1024),\n",
    "            activation,\n",
    "            dropout,\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.GroupNorm(num_groups,512),\n",
    "            activation,\n",
    "            dropout,\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.GroupNorm(num_groups,256),\n",
    "            activation,\n",
    "            dropout,\n",
    "            nn.Linear(in_features=256, out_features=n_outputs),\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        logits = self.network(X)\n",
    "        return logits\n",
    "    \n",
    "    def save(self, name):\n",
    "        T.save(self.state_dict(), f\"{self.save_dir}/{name}.pth\")\n",
    "\n",
    "    def load(self, name):\n",
    "        self.load_state_dict(T.load(f\"{self.save_dir}/{name}.pth\"))\n",
    "\n",
    "n_inputs = X_train_PCA.shape[1] # 140 inputs\n",
    "n_outputs = 21 # 21 labels\n",
    "\n",
    "# Move a tensor to the GPU\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model_1 = NeuralNetwork(n_inputs=n_inputs, n_outputs=n_outputs, p_dropout=0.5).to(device)\n",
    "model_2 = NeuralNetwork(n_inputs=n_inputs, n_outputs=n_outputs, p_dropout=0.5).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(y_pred, y_true, labels):\n",
    "    N = labels.shape[0]\n",
    "    matrix = [[0] * (N + 1) for _ in range(N + 1)]\n",
    "\n",
    "    matrix[0][0] = \" \"\n",
    "    for i in range(1, N):\n",
    "        matrix[i][0] = f\"{i}\"\n",
    "        matrix[0][i] = f\"{i}\"\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        matrix[round(y_pred[i]) + 1][y_true[i] + 1] += 1\n",
    "\n",
    "    for i in range(N):\n",
    "        print(\" \".join(map(str, matrix[i])))\n",
    "\n",
    "    return sum([matrix[i + 1][i + 1] for i in range(2)]) / len(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model1, model2, X_val, y_val, criterion):\n",
    "    size = len(y_val)\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    with T.no_grad():\n",
    "        X = T.from_numpy(X_val).to(T.float32).to(device)\n",
    "        y_true = T.Tensor(y_val).to(T.float).to(device)\n",
    "\n",
    "        logits_1 = model1.forward(X)\n",
    "        logits_2 = model2.forward(X)\n",
    "\n",
    "        loss_1, loss_2 = criterion(logits_1, logits_2, y_true, 0.2)\n",
    "\n",
    "        correct_1 = (logits_1.argmax(1) == y_true.argmax(1)).type(T.float).sum().item()\n",
    "        correct_2 = (logits_2.argmax(1) == y_true.argmax(1)).type(T.float).sum().item()\n",
    "        \n",
    "        loss_1 /= size\n",
    "        loss_2 /= size\n",
    "        accuracy_1 = correct_1/size\n",
    "        accuracy_2 = correct_2/size\n",
    "        print(f\"Validation Error (Model 1): \\n Accuracy: {(100 * (accuracy_1)):>0.1f}%, Avg loss: {loss_1:>8f}\")\n",
    "        print(f\"Validation Error (Model 2): \\n Accuracy: {(100 * (accuracy_2)):>0.1f}%, Avg loss: {loss_2:>8f} \\n\")\n",
    "    \n",
    "    return accuracy_1, accuracy_2, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_1, model_2, X_train, y_train, criterion, optimizer_1, optimizer_2):\n",
    "    size = len(X_train)\n",
    "    batch_size = 141\n",
    "\n",
    "    #Prevents model from memorizing the position of data\n",
    "    indices = np.random.randint(0, size, size)\n",
    "\n",
    "    model_1.train()\n",
    "    model_2.train()\n",
    "\n",
    "    for i in range(size//batch_size):\n",
    "        start = batch_size * i\n",
    "        end = start + batch_size\n",
    "\n",
    "        X = T.from_numpy(X_train[indices[start:end]]).to(T.float32).to(device)\n",
    "        y_true = T.Tensor(y_train[indices[start:end]]).to(T.float).to(device)\n",
    "\n",
    "        logits_1 = model_1.forward(X)\n",
    "        logits_2 = model_2.forward(X)\n",
    "\n",
    "        loss_1, loss_2 = criterion(logits_1, logits_2, y_true, 0.2)\n",
    "\n",
    "        # Gradiant Descent using Adam optimizer for best performance\n",
    "        optimizer_1.zero_grad()\n",
    "        loss_1.backward()\n",
    "        optimizer_1.step()\n",
    "\n",
    "        optimizer_2.zero_grad()\n",
    "        loss_2.backward()\n",
    "        optimizer_2.step()\n",
    "\n",
    "        correct_1 = (logits_1.argmax(1) == y_true.argmax(1)).type(T.float).sum().item()\n",
    "        correct_2 = (logits_2.argmax(1) == y_true.argmax(1)).type(T.float).sum().item()\n",
    "\n",
    "        accuracy_1 = correct_1/batch_size\n",
    "        accuracy_2 = correct_2/batch_size\n",
    "\n",
    "        # if (i * batch_size) % 564 == 0:\n",
    "        #     loss_1, loss_2, current = loss_1.item(), loss_2.item(), (i + 1) * batch_size\n",
    "        #     print(f\"Accuracy_1: {(100 * (accuracy_1)):>0.1f}%, Loss_1: {loss_1:>7f}, \", end=\"\")\n",
    "        #     print(f\"Accuracy_2: {(100 * (accuracy_2)):>0.1f}% Loss_2: {loss_2:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Loss functions\n",
    "def loss_coteaching(y_1, y_2, t, forget_rate):\n",
    "    loss_1 = F.cross_entropy(y_1, t, reduction=\"none\")\n",
    "    ind_1_sorted = T.argsort(loss_1.data)\n",
    "    loss_1_sorted = loss_1[ind_1_sorted]\n",
    "\n",
    "    loss_2 = F.cross_entropy(y_2, t, reduction=\"none\")\n",
    "    ind_2_sorted = T.argsort(loss_2.data)\n",
    "    loss_2_sorted = loss_2[ind_2_sorted]\n",
    "\n",
    "    remember_rate = 1 - forget_rate\n",
    "    num_remember = int(remember_rate * len(loss_1_sorted))\n",
    "\n",
    "\n",
    "    ind_1_update=ind_1_sorted[:num_remember]\n",
    "    ind_2_update=ind_2_sorted[:num_remember]\n",
    "    # exchange\n",
    "    loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update])\n",
    "    loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update])\n",
    "\n",
    "    return T.sum(loss_1_update)/num_remember, T.sum(loss_2_update)/num_remember\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[218], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_PCA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m     accuracy_1, accuracy_2, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m validate_model(model_1, model_2, X_val_PCA, y_val, criterion)\n",
      "Cell \u001b[0;32mIn[215], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_1, model_2, X_train, y_train, criterion, optimizer_1, optimizer_2)\u001b[0m\n\u001b[1;32m     15\u001b[0m X \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mfrom_numpy(X_train[indices[start:end]])\u001b[38;5;241m.\u001b[39mto(T\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m y_true \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mTensor(y_train[indices[start:end]])\u001b[38;5;241m.\u001b[39mto(T\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 18\u001b[0m logits_1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m logits_2 \u001b[38;5;241m=\u001b[39m model_2\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[1;32m     21\u001b[0m loss_1, loss_2 \u001b[38;5;241m=\u001b[39m criterion(logits_1, logits_2, y_true, \u001b[38;5;241m0.2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[212], line 30\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 30\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/dropout.py:234\u001b[0m, in \u001b[0;36mAlphaDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:1264\u001b[0m, in \u001b[0;36malpha_dropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[0;32m-> 1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39malpha_dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Global Variables\n",
    "epochs = 5_000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Load Pre-Trained Models\n",
    "# model_1.load(\"NeuralNetwork-1_acc-50.29_loss-0.000003\")\n",
    "# model_2.load(\"NeuralNetwork-2_acc-50.38_loss-0.000003\")\n",
    "\n",
    "criterion = loss_coteaching # Co-teaching loss function\n",
    "optimizer_1 = optim.Adam(model_1.parameters(), lr=learning_rate)\n",
    "optimizer_2 = optim.Adam(model_2.parameters(), lr=learning_rate)\n",
    "\n",
    "best_accuracy = 0.61 # ???\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "\n",
    "    train_model(model_1, model_2, X_train_PCA, y_train, criterion, optimizer_1, optimizer_2)\n",
    "    print('Finished training')\n",
    "    \n",
    "    accuracy_1, accuracy_2, loss_1, loss_2 = validate_model(model_1, model_2, X_val_PCA, y_val, criterion)\n",
    "\n",
    "    if max(accuracy_1, accuracy_2) > best_accuracy:\n",
    "        print(f\"[+] Saving Model...\")\n",
    "\n",
    "        model_1.save(f\"NeuralNetwork-1_acc-{accuracy_1 * 100:.2f}_loss-{loss_1:>8f}\")\n",
    "        model_2.save(f\"NeuralNetwork-2_acc-{accuracy_2 * 100:.2f}_loss-{loss_2:>8f}\")\n",
    "        best_accuracy = max(accuracy_1, accuracy_2)\n",
    "\n",
    "        print(f\"[!] Models Saved.\")\n",
    "\n",
    "    epoch += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = loss_coteaching\n",
    "\n",
    "def test_model(model_1, model_2, X_test, y_test, criterion):\n",
    "    size = len(y_test)\n",
    "\n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "    with T.no_grad():\n",
    "        X = T.from_numpy(X_test).to(T.float32).to(device)\n",
    "        y_true = T.Tensor(y_test).to(T.float).to(device)\n",
    "\n",
    "        logits_1 = model_1.forward(X)\n",
    "        logits_2 = model_2.forward(X)\n",
    "\n",
    "        loss_1, loss_2 = criterion(logits_1, logits_2, y_true, 0.2)\n",
    "\n",
    "        correct_1 = (logits_1.argmax(1) == y_true.argmax(1)).type(T.float).sum().item()\n",
    "        correct_2 = (logits_2.argmax(1) == y_true.argmax(1)).type(T.float).sum().item()\n",
    "        \n",
    "        loss_1 /= size\n",
    "        loss_2 /= size\n",
    "        accuracy_1 = correct_1/size\n",
    "        accuracy_2 = correct_2/size\n",
    "        \n",
    "        print(f\"Test Error (Model 1): \\n Accuracy: {(100 * (accuracy_1)):>0.1f}%, Avg loss: {loss_1:>8f}\")\n",
    "        print(f\"Test Error (Model 2): \\n Accuracy: {(100 * (accuracy_2)):>0.1f}%, Avg loss: {loss_2:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error (Model 1): \n",
      " Accuracy: 62.4%, Avg loss: 0.000009\n",
      "Test Error (Model 2): \n",
      " Accuracy: 61.6%, Avg loss: 0.000004 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1.load(\"NeuralNetwork-1_acc-61.81_loss-0.000009\")\n",
    "model_2.load(\"NeuralNetwork-2_acc-61.62_loss-0.000004\")\n",
    "test_model(model_1, model_2, X_test_PCA, y_test, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
